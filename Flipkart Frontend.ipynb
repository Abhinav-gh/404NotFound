{"cells":[{"cell_type":"markdown","metadata":{"id":"PFop7A1Vfvjm"},"source":["# 1. Install Gradio and Required Libraries\n","### Start by installing Gradio if it's not already installed."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ewtMbahKcF3w"},"outputs":[],"source":["! pip install gradio\n","! pip install cv\n","! pip install ultralytics\n","! pip install supervision"]},{"cell_type":"markdown","metadata":{"id":"SRN1sfE1f83w"},"source":["# 2. Import Libraries\n","### Getting all the necessary Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22052,"status":"ok","timestamp":1729193625536,"user":{"displayName":"transyltooniaa","userId":"16629425991490611627"},"user_tz":-330},"id":"UAv0vzXggDUv","outputId":"da88f7b5-e853-43d1-fef3-0aab13d5bd33"},"outputs":[{"output_type":"stream","name":"stdout","text":["Creating new Ultralytics Settings v0.0.6 file âœ… \n","View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n","Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"]}],"source":["import gradio as gr\n","import random\n","import numpy as np\n","from PIL import Image\n","import cv2\n","import time\n","from ultralytics import YOLO\n","import supervision as sv\n","import pandas as pd\n","from google.colab.patches import cv2_imshow\n","from IPython.display import clear_output\n","from collections import defaultdict, deque"]},{"cell_type":"markdown","metadata":{"id":"VBUU8AruRPZh"},"source":["# 3. Import Drive\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19790,"status":"ok","timestamp":1729193645322,"user":{"displayName":"transyltooniaa","userId":"16629425991490611627"},"user_tz":-330},"id":"vhR2uY9bRTau","outputId":"a879c4c4-c67a-48d1-97df-07f02f466d78"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"qiZYtWJSgHK2"},"source":["# 4. Brand Recognition Backend\n","\n"]},{"cell_type":"markdown","metadata":{"id":"u0MRv4UcQ-1h"},"source":["### Model for Grocery Detection"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"js_xQ6MQQ9fk"},"outputs":[],"source":["model = YOLO('/content/drive/MyDrive/kitkat_s.pt')"]},{"cell_type":"markdown","metadata":{"id":"aOQpUpnWQZs8"},"source":["### Image uploading for Grocery detection"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ARthG_EiPujD"},"outputs":[],"source":["def detect_grocery_items(image):\n","    image = np.array(image)[:, :, ::-1]\n","    results = model(image)\n","    annotated_image = results[0].plot()\n","\n","    class_ids = results[0].boxes.cls.cpu().numpy()\n","    confidences = results[0].boxes.conf.cpu().numpy()\n","\n","    threshold = 0.4\n","    class_counts = {}\n","    class_confidences = {}\n","\n","    for i, class_id in enumerate(class_ids):\n","        confidence = confidences[i]\n","        if confidence >= threshold:\n","            class_name = model.names[int(class_id)]\n","\n","            if class_name in class_counts:\n","                class_counts[class_name] += 1\n","            else:\n","                class_counts[class_name] = 1\n","\n","            if class_name in class_confidences:\n","                class_confidences[class_name].append(confidence)\n","            else:\n","                class_confidences[class_name] = [confidence]\n","\n","    if not class_counts:\n","        return image, [], \"The model failed to recognize items or the image may contain untrained objects.\"\n","\n","    summary_table = [[class_name, count, f\"{np.mean(class_confidences[class_name]):.2f}\"]\n","                     for class_name, count in class_counts.items()]\n","\n","    annotated_image_rgb = annotated_image[:, :, ::-1]\n","    return annotated_image_rgb, summary_table, \"Object Recognised Successfully ðŸ¥³ \"\n"]},{"cell_type":"markdown","metadata":{"id":"EDw4WZqQQhvH"},"source":["### Detect Grovcery brand from video"]},{"cell_type":"code","source":["def iou(box1, box2):\n","    # Calculate intersection over union\n","    x1 = max(box1[0], box2[0])\n","    y1 = max(box1[1], box2[1])\n","    x2 = min(box1[2], box2[2])\n","    y2 = min(box1[3], box2[3])\n","\n","    intersection = max(0, x2 - x1) * max(0, y2 - y1)\n","    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n","    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n","\n","    iou = intersection / float(area1 + area2 - intersection)\n","    return iou"],"metadata":{"id":"6RIf4Mr91Woi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def smooth_box(box_history):\n","    if not box_history:\n","        return None\n","    return np.mean(box_history, axis=0)"],"metadata":{"id":"iRc7Or0d1Z0N"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uw6XOmA3gTKH"},"outputs":[],"source":["def process_video(input_path, output_path):\n","    cap = cv2.VideoCapture(input_path)\n","\n","    # Get video properties\n","    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","    fps = int(cap.get(cv2.CAP_PROP_FPS))\n","\n","    # Initialize video writer\n","    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n","    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n","\n","    # Initialize variables for tracking\n","    detected_items = {}\n","    frame_count = 0\n","\n","    # For result confirmation\n","    detections_history = defaultdict(lambda: defaultdict(int))\n","\n","    while cap.isOpened():\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","\n","        frame_count += 1\n","\n","        # Run YOLO detection every 5th frame\n","        if frame_count % 5 == 0:\n","            results = model(frame)\n","\n","            current_frame_detections = []\n","\n","            for r in results:\n","                boxes = r.boxes\n","                for box in boxes:\n","                    x1, y1, x2, y2 = box.xyxy[0].tolist()\n","                    conf = box.conf.item()\n","                    cls = int(box.cls.item())\n","                    brand = model.names[cls]\n","\n","                    current_frame_detections.append((brand, [x1, y1, x2, y2], conf))\n","\n","            # Match current detections with existing items\n","            for brand, box, conf in current_frame_detections:\n","                matched = False\n","                for item_id, item_info in detected_items.items():\n","                    if iou(box, item_info['smoothed_box']) > 0.5:\n","                        item_info['frames_detected'] += 1\n","                        item_info['total_conf'] += conf\n","                        item_info['box_history'].append(box)\n","                        if len(item_info['box_history']) > 10:\n","                            item_info['box_history'].popleft()\n","                        item_info['smoothed_box'] = smooth_box(item_info['box_history'])\n","                        item_info['last_seen'] = frame_count\n","                        matched = True\n","                        break\n","\n","                if not matched:\n","                    item_id = len(detected_items)\n","                    detected_items[item_id] = {\n","                        'brand': brand,\n","                        'box_history': deque([box], maxlen=10),\n","                        'smoothed_box': box,\n","                        'frames_detected': 1,\n","                        'total_conf': conf,\n","                        'last_seen': frame_count\n","                    }\n","\n","                detections_history[brand][frame_count] += 1\n","\n","\n","        for item_id, item_info in list(detected_items.items()):\n","            if frame_count - item_info['last_seen'] > fps * 2:  # 2 seconds\n","                del detected_items[item_id]\n","                continue\n","\n","            # Interpolate box position\n","            if item_info['smoothed_box'] is not None:\n","                alpha = 0.3\n","                current_box = item_info['smoothed_box']\n","                target_box = item_info['box_history'][-1] if item_info['box_history'] else current_box\n","                interpolated_box = [\n","                    current_box[i] * (1 - alpha) + target_box[i] * alpha\n","                    for i in range(4)\n","                ]\n","                item_info['smoothed_box'] = interpolated_box\n","\n","                x1, y1, x2, y2 = map(int, interpolated_box)\n","                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n","                cv2.putText(frame, f\"{item_info['brand']}\",\n","                            (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n","\n","        out.write(frame)\n","\n","    cap.release()\n","    out.release()\n","\n","    # Calculate final counts and confirm results\n","    total_frames = frame_count\n","    confirmed_items = {}\n","    for brand, frame_counts in detections_history.items():\n","        detection_frames = len(frame_counts)\n","        if detection_frames > total_frames * 0.1:\n","            avg_count = sum(frame_counts.values()) / detection_frames\n","            confirmed_items[brand] = round(avg_count)\n","\n","    return confirmed_items"]},{"cell_type":"code","source":["def annotate_video(input_video):\n","    output_path = 'annotated_output.mp4'\n","    confirmed_items = process_video(input_video, output_path)\n","\n","    item_list = [(brand, quantity) for brand, quantity in confirmed_items.items()]\n","\n","    status_message = \"Video processed successfully!\"\n","\n","    return output_path, item_list, status_message"],"metadata":{"id":"Lc2w5YVKnTjC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ix5AXFDbQ32Y"},"source":["# 5. OCR Backend\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NWo9C1OIDvIP"},"outputs":[],"source":["def process_OCR(image):\n","    image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n","    results = model(image)\n","\n","    if isinstance(results, list):\n","        results = results[0]\n","\n","    result = {\n","        \"boxes\": [],\n","        \"class_names\": [],\n","        \"confidences\": []\n","    }\n","\n","    for box in results.boxes:\n","        result[\"boxes\"].append(box.xyxy.tolist())\n","        result[\"confidences\"].append(box.conf.item())\n","        result[\"class_names\"].append(results.names[int(box.cls)])\n","\n","    result_image = results.plot()\n","    result_image = cv2.cvtColor(result_image, cv2.COLOR_BGR2RGB)\n","\n","    return result_image, result\n","\n","# Dummy function for detect_and_ocr\n","def detect_and_ocr(image):\n","    # Simulate the object detection and OCR process\n","    # Dummy bounding box image (for illustration)\n","    result_image = np.array(image)  # Just return the original image in this dummy function\n","\n","    # Dummy extracted text (you'll replace this with real OCR output)\n","    extracted_text = \"Detected Text: Example Text from Image\"\n","\n","    # Dummy refined text (you'll replace this with your Gemini processing result)\n","    refined_text = \"Refined Text: Cleaned up text\"\n","\n","    # Dummy validated output (this will be the validated text from further steps)\n","    validated_text = \"Validated Output: Verified text after review\"\n","\n","    return result_image, extracted_text, refined_text, validated_text\n","\n","# Dummy function for further_processing\n","def further_processing(image, extracted_text):\n","    # Simulate further processing on the extracted text\n","    refined_text = f\"Further refined: {extracted_text} (Refined Again)\"\n","    return refined_text\n","\n","# Dummy function for handle_processing\n","def handle_processing(validated_text):\n","    # Simulate checking the validated text and showing/hiding the further processing button\n","    # Here, we show the button if the validated text is non-empty\n","    if validated_text.strip():\n","        return gr.update(visible=True)\n","    else:\n","        return gr.update(visible=False)\n"]},{"cell_type":"markdown","metadata":{"id":"DV0ogUeegaAb"},"source":["# 5. Frontend Of Brand Recognition"]},{"cell_type":"markdown","metadata":{"id":"BhTIOB_vSsbV"},"source":["## Layout for Image interface"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vJdfukblgbIf"},"outputs":[],"source":["def create_image_interface():\n","    return gr.Interface(\n","        fn=detect_grocery_items,\n","        inputs=gr.Image(label=\"Upload Image\", height=400, width=400),\n","        outputs=[\n","            gr.Image(label=\"Image with Bounding Boxes\", height=400, width=400),\n","            gr.Dataframe(headers=[\"Item\", \"Quantity\", \"Avg Confidence\"], label=\"Detected Items and Quantities\", elem_id=\"summary_table\"),\n","            gr.Textbox(label=\"Status\", elem_id=\"status_message\")\n","        ],\n","        title=\"Grocery Item Detection in an Image\",\n","        description=\"Upload an image for object detection. The model will return an annotated image, item quantities, and average confidence scores.\",\n","        css=\".gr-table { font-size: 16px; text-align: left; width: 50%; margin: auto; } #summary_table { margin-top: 20px; }\"\n","    )"]},{"cell_type":"markdown","metadata":{"id":"ifKhlFkZgie2"},"source":["## Layout For Video Interface"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aH4yO7kygkp3"},"outputs":[],"source":["def create_video_interface():\n","    return gr.Interface(\n","        fn=annotate_video,  # This is the function that processes the video and returns the results\n","        inputs=gr.Video(label=\"Upload Video\", height=400, width=400),\n","        outputs=[\n","            gr.Video(label=\"Annotated Video\", height=400, width=400),  # To display the annotated video\n","            gr.Dataframe(headers=[\"Item\", \"Quantity\"], label=\"Detected Items and Quantities\", elem_id=\"summary_table\"),\n","            gr.Textbox(label=\"Status\", elem_id=\"status_message\")  # Any additional status messages\n","        ],\n","        title=\"Grocery Item Detection in a Video\",\n","        description=\"Upload a video for object detection. The model will return an annotated video with bounding boxes and item quantities. Low confidence values may indicate incorrect detection.\",\n","        css=\"\"\"\n","            .gr-table { font-size: 16px; text-align: left; width: 50%; margin: auto; }\n","            #summary_table { margin-top: 20px; }\n","        \"\"\"\n","    )"]},{"cell_type":"code","source":["def create_brand_recog_interface():\n","    with gr.Blocks() as demo:\n","        gr.Markdown(\"# Flipkart Grid Robotics Track - Brand Recognition Interface\")\n","\n","        with gr.Tabs():\n","            with gr.Tab(\"Image\"):\n","                create_image_interface()\n","            with gr.Tab(\"Video\"):\n","                create_video_interface()\n","    return demo\n","\n","Brand_recog = create_brand_recog_interface()\n"],"metadata":{"id":"1vmcJbgmrYl8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sKhGxo8F-T9y"},"source":["# Frontend Of OCR"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BSb-LkI2-TTJ"},"outputs":[],"source":["def create_ocr_interface():\n","    with gr.Blocks() as ocr_interface:\n","        gr.Markdown(\"# Flipkart Grid Robotics Track - OCR Interface\")\n","\n","        with gr.Tabs():\n","            with gr.TabItem(\"Upload & Detection\"):\n","                with gr.Row():\n","                    input_image = gr.Image(type=\"pil\", label=\"Upload Image\", height=400, width=400)\n","                    output_image = gr.Image(label=\"Image with Bounding Boxes\", height=400, width=400)\n","\n","                btn = gr.Button(\"Detect and OCR\")\n","\n","            with gr.TabItem(\"OCR Results\"):\n","                with gr.Row():\n","                    extracted_textbox = gr.Textbox(label=\"Extracted Text\", lines=5)\n","                with gr.Row():\n","                    refined_textbox = gr.Textbox(label=\"Refined Text from Gemini\", lines=5)\n","                with gr.Row():\n","                    validated_textbox = gr.Textbox(label=\"Validated Output\", lines=5)\n","\n","                further_button = gr.Button(\"Further Processing\", visible=False)\n","\n","        # Button click event for OCR detection\n","        btn.click(\n","            detect_and_ocr,\n","            inputs=[input_image],\n","            outputs=[output_image, extracted_textbox, refined_textbox, validated_textbox]\n","        )\n","\n","        # Further processing button click event\n","        further_button.click(\n","            further_processing,\n","            inputs=[input_image, extracted_textbox],\n","            outputs=refined_textbox\n","        )\n","\n","        # Monitor validated output to control button visibility\n","        validated_textbox.change(\n","            handle_processing,\n","            inputs=[validated_textbox],\n","            outputs=[further_button],\n","        )\n","\n","    return ocr_interface\n","\n","# Create the OCR interface\n","ocr_interface = create_ocr_interface()\n"]},{"cell_type":"markdown","metadata":{"id":"J8OSFWUxgpkK"},"source":["# 6. Create a Tabbed Interface for Both Image and Video\n","### Here, we combine the image and video interfaces into a tabbed structure so users can switch between them easily."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uU5PErJWgx2A"},"outputs":[],"source":["def create_tabbed_interface():\n","    return gr.TabbedInterface(\n","        [Brand_recog,  ocr_interface ],\n","        [\"Brand Recongnition\", \"OCR\"]\n","    )\n","\n","tabbed_interface = create_tabbed_interface()"]},{"cell_type":"markdown","metadata":{"id":"iZoOhTz_fuhC"},"source":[]},{"cell_type":"markdown","metadata":{"id":"PXib0pFpg5U9"},"source":["# 7. Launch the Gradio Interface\n","### Finally, launch the Gradio interface to make it interactable."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":645},"id":"A4GOdtXag7uc","executionInfo":{"status":"ok","timestamp":1729193845480,"user_tz":-330,"elapsed":1631,"user":{"displayName":"transyltooniaa","userId":"16629425991490611627"}},"outputId":"58d78925-e850-452e-bd9c-8c5e4b8a131e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://c034444fe19ff19929.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://c034444fe19ff19929.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":28}],"source":["tabbed_interface.launch()"]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}