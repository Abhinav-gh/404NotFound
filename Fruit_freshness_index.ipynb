{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 9643655,
          "sourceType": "datasetVersion",
          "datasetId": 5879812
        }
      ],
      "dockerImageVersionId": 30786,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "LG3igBf7q6_5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fruit Freshness Index\n",
        "\n",
        "This notebook attempts to determine a freshness index provided an image of a fruit.\n",
        "\n",
        "**How the algorithm works?**\n",
        "\n",
        "Suppose we are training a model to determine the freshness index of a banana.\n",
        "1. We use a collection of rotten banana images.\n",
        "2. These images are passed to an *EfficientNet* model and the features are extracted from the dataset.\n",
        "4. Now we have a distribution which essentially represents the collection of rotten bananas.\n",
        "5. Now we find the **mean** and **covariance matrix** of this distribution.\n",
        "6. Suppose we have a test banana. We calculate the **Mahalanobis Distance** of the banana from the distribution. For more reference on Mahalanobis distance: https://www.machinelearningplus.com/statistics/mahalanobis-distance/\n",
        "7. The more the Mahalanobis distance implies the furthur the test point is from the distribution, implies that the banana is more fresh.\n",
        "8. That is how we classify the freshness index.\n",
        "\n",
        "### **Subsequent improvements to the model:**\n",
        "\n",
        "The model uses EfficientNet to extract the features. EfficientNet has been chosen as it is real time. Furthur it is a CNN architecture so it is expected that the image features extracted won't depend on the camera angle. The drawback is that the model draws all information including the background information. So some predictions are not that accurate. The workaround is to use Segmentation Techniques and Vision transformers to extract only what is necessary."
      ],
      "metadata": {
        "id": "GXF92gPsq6_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing torch and setting up device agnostic code\n",
        "import torch\n",
        "\n",
        "# Set up device agnostic code\n",
        "\n",
        "# Check if GPU is available and send the model to GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "trusted": true,
        "id": "4yKWPa1Sq6_9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Extraction using EfficientNet\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "class EfficientNet_FeatureExtractor(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(EfficientNet_FeatureExtractor, self).__init__()\n",
        "        # Load a pre-trained EfficientNet model (e.g., EfficientNet-B0)\n",
        "        self.efficientnet = models.efficientnet_b0(pretrained=True)\n",
        "\n",
        "        # Remove the classifier (the last fully connected layer)\n",
        "        self.efficientnet = nn.Sequential(*list(self.efficientnet.children())[:-1])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through EfficientNet until before the classifier\n",
        "        x = self.efficientnet(x)\n",
        "\n",
        "        # Flatten the output\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Example usage:\n",
        "# model = EfficientNet_FeatureExtractor()\n",
        "# features = model(input_tensor)"
      ],
      "metadata": {
        "trusted": true,
        "id": "9lfG-ru_q6_-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model = EfficientNet_FeatureExtractor()\n",
        "model"
      ],
      "metadata": {
        "trusted": true,
        "id": "LU6q3hh9q6_-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the mean and variance of the images whose features will be extracted\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "\n",
        "# Define a transform to convert images to tensors without normalizing them\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),        # Resize the shorter side to 256 pixels\n",
        "    transforms.CenterCrop(224),    # Crop the center to get a 224x224 image\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Load your dataset\n",
        "# Assuming you have a folder containing your custom images (custom_dataset_folder)\n",
        "dataset = datasets.ImageFolder(root='/kaggle/input/bananas-dataset/Dataset', transform=transform)\n",
        "\n",
        "# Create a DataLoader\n",
        "loader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Initialize variables to calculate the mean and std\n",
        "mean = 0.0\n",
        "std = 0.0\n",
        "total_images = 0\n",
        "\n",
        "# Iterate over the dataset to compute mean and std\n",
        "for images, _ in loader:\n",
        "    batch_samples = images.size(0)  # Number of images in the batch\n",
        "    images = images.view(batch_samples, images.size(1), -1)  # Flatten each image (C, H*W)\n",
        "\n",
        "    # Calculate mean and std for this batch and add to the running total\n",
        "    mean += images.mean(2).sum(0)  # Mean across the H*W dimensions\n",
        "    std += images.std(2).sum(0)    # Std across the H*W dimensions\n",
        "    total_images += batch_samples\n",
        "\n",
        "# Final mean and std across all images in the dataset\n",
        "mean /= total_images\n",
        "std /= total_images\n",
        "\n",
        "print(f\"Mean: {mean}\")\n",
        "print(f\"Std: {std}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "3A2iv8EYq6__"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you calculated mean and std as follows\n",
        "# mean = [0.7245, 0.6862, 0.6531]  # Your calculated values\n",
        "# std = [0.2138, 0.2454, 0.3047]   # Your calculated values\n",
        "\n",
        "# Transforming the images into the format so that they can be passes through the EfficientNet model\n",
        "\n",
        "# Define the transform for your dataset, including normalization with custom mean and std\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean, std=std)  # Use your custom mean and std\n",
        "])\n",
        "\n",
        "# Load the dataset with the updated transform\n",
        "test_dataset = datasets.ImageFolder(root='/kaggle/input/bananas-dataset/Dataset', transform=transform)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "BUCv8gvSq6__"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset"
      ],
      "metadata": {
        "trusted": true,
        "id": "yqtwdd76q7AA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting features from Efficientnet model\n",
        "\n",
        "\n",
        "# Initialize the feature extractor model\n",
        "model = EfficientNet_FeatureExtractor().to(device)\n",
        "model.eval()  # Set to evaluation mode\n",
        "\n",
        "# Create a DataLoader for the test dataset\n",
        "test_loader = DataLoader(test_dataset, batch_size=50, shuffle=False)\n",
        "\n",
        "\n",
        "# Store the extracted features\n",
        "all_features = []\n",
        "\n",
        "# Loop over the test dataset and extract features\n",
        "with torch.no_grad():  # Disable gradient calculation for efficiency\n",
        "    for images, _ in test_loader:\n",
        "        # Send the images to the same device as the model\n",
        "        images = images.to(device)\n",
        "\n",
        "        # Pass the images through the feature extractor\n",
        "        features = model(images)\n",
        "\n",
        "        # Move features to CPU and convert to NumPy (optional)\n",
        "        features = features.cpu().numpy()\n",
        "\n",
        "        # Append the features for further use\n",
        "        all_features.append(features)\n",
        "\n",
        "# Print the shape of each batch stored in the list\n",
        "for i, features in enumerate(all_features):\n",
        "    print(f\"Shape of batch {i}: {features.shape}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "Hne9kimAq7AB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the mean and varinance of the entire distribution\n",
        "\n",
        "import torch\n",
        "\n",
        "# Stack all the feature vectors into a single tensor\n",
        "all_features_tensor = torch.cat([torch.tensor(batch) for batch in all_features], dim=0)\n",
        "\n",
        "# Calculate the mean and variance along the feature dimension\n",
        "# Feature dimension is typically the second axis (axis 1) in the feature vectors\n",
        "feature_mean = all_features_tensor.mean(dim=0)\n",
        "feature_mean = feature_mean.to(device)\n",
        "feature_variance = all_features_tensor.var(dim=0)\n",
        "\n",
        "print(f\"Feature Mean Shape: {feature_mean.shape}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "hID1BpWRq7AB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the Covariance Matrix of the distribution\n",
        "\n",
        "import torch\n",
        "\n",
        "# Convert the list of arrays into one large tensor\n",
        "all_features_tensor = torch.cat([torch.tensor(f) for f in all_features], dim=0)\n",
        "\n",
        "# Move the stacked tensor to the device (if you're using GPU)\n",
        "all_features_tensor = all_features_tensor.to(device)\n",
        "\n",
        "# Calculate the mean along the batch dimension\n",
        "feature_mean_temp = all_features_tensor.mean(dim=0)\n",
        "\n",
        "# Center the feature vectors by subtracting the mean\n",
        "centered_features = all_features_tensor - feature_mean_temp\n",
        "\n",
        "# Calculate the covariance matrix\n",
        "# Covariance matrix: (num_features, num_features)\n",
        "covariance_matrix = torch.cov(centered_features.T)\n",
        "covariance_matrix = covariance_matrix.to(device)\n",
        "\n",
        "print(f\"All Feature Tensor Shape: {all_features_tensor.shape}\")\n",
        "print(f\"Covariance Matrix Shape: {covariance_matrix.shape}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "YXF1AeWyq7AB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the function to calculate the Mahalanobis distance\n",
        "\n",
        "import torch\n",
        "\n",
        "def mahalanobis(x=None, feature_mean=None, feature_cov=None):\n",
        "    \"\"\"Compute the Mahalanobis Distance between each row of x and the data\n",
        "    x             : tensor of shape [batch_size, num_features], feature vectors of test data\n",
        "    feature_mean  : tensor of shape [num_features], mean of the training feature vectors\n",
        "    feature_cov   : tensor of shape [num_features, num_features], covariance matrix of the training feature vectors\n",
        "    \"\"\"\n",
        "\n",
        "    # Subtract the mean from x\n",
        "    x_minus_mu = x - feature_mean\n",
        "\n",
        "    # Invert the covariance matrix\n",
        "    inv_covmat = torch.inverse(feature_cov)\n",
        "\n",
        "    # Mahalanobis distance computation: (x - mu)^T * inv_cov * (x - mu)\n",
        "    left_term = torch.matmul(x_minus_mu, inv_covmat)  # Shape: [batch_size, num_features]\n",
        "    mahal = torch.matmul(left_term, x_minus_mu.T)     # Shape: [batch_size, batch_size]\n",
        "\n",
        "    # Return the diagonal elements which are the distances for each sample\n",
        "    return mahal.diag()\n",
        "\n",
        "# Example usage:\n",
        "# x, feature_mean, and feature_cov should all be PyTorch tensors\n",
        "# x: shape [batch_size, num_features]\n",
        "# feature_mean: shape [num_features]\n",
        "# feature_cov: shape [num_features, num_features]"
      ],
      "metadata": {
        "trusted": true,
        "id": "FFnyrxf9q7AC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing on a single image\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "# Define the transform (same as before)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean, std=std)  # Use your custom mean and std\n",
        "])\n",
        "\n",
        "# Load the image\n",
        "img_path = '/kaggle/input/fresh-and-stale-classification/dataset/Train/rottenbanana/Screen Shot 2018-06-12 at 8.50.20 PM.png'  # Example image path\n",
        "img = Image.open(img_path)\n",
        "\n",
        "# Print image mode\n",
        "print(f\"Image mode: {img.mode}\")\n",
        "\n",
        "# If the image is in RGBA or another format, convert to RGB\n",
        "if img.mode != 'RGB':\n",
        "    img = img.convert('RGB')\n",
        "\n",
        "\n",
        "# Apply the transformation\n",
        "img_transformed = transform(img)\n",
        "\n",
        "# If needed, add a batch dimension (because models expect batches)\n",
        "img_transformed = img_transformed.unsqueeze(0)  # Shape: [1, 3, 224, 224]\n",
        "\n",
        "# Move the transformed image to the appropriate device (GPU/CPU)\n",
        "img_transformed = img_transformed.to(device)\n",
        "\n",
        "# Ensure the model is on the same device\n",
        "model = model.to(device)\n",
        "\n",
        "# Set model to eval mode for feature extraction\n",
        "model.eval()\n",
        "\n",
        "# Pass the images through the feature extractor (no gradient needed)\n",
        "with torch.no_grad():\n",
        "    features_1 = model(img_transformed)\n",
        "\n",
        "# Move the features to the same device as feature_mean and covariance_matrix (if needed)\n",
        "features_1 = features_1.to(device)\n",
        "\n",
        "# Ensure feature_mean and covariance_matrix are also on the correct device\n",
        "feature_mean = feature_mean.to(device)\n",
        "covariance_matrix = covariance_matrix.to(device)\n",
        "\n",
        "# Calculate the Mahalanobis distance for the feature vector\n",
        "distance = mahalanobis(features_1, feature_mean, covariance_matrix)\n",
        "distance = torch.abs(distance) / 1e8"
      ],
      "metadata": {
        "trusted": true,
        "id": "qCqd6X-Aq7AC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_banana_by_distance(distance):\n",
        "    \"\"\"\n",
        "    Classifies the banana's freshness based on the Mahalanobis distance.\n",
        "\n",
        "    Args:\n",
        "        distance (float): Mahalanobis distance of the banana.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the classification and relevant details.\n",
        "    \"\"\"\n",
        "\n",
        "    # Define thresholds for classification based on the provided distances\n",
        "    if distance >= 9:\n",
        "        # Case 1: Completely Fresh Banana\n",
        "        return {\n",
        "            \"Classification\": \"Completely Fresh\",\n",
        "            \"Freshness Index\": 10,\n",
        "            \"Color\": \"Mostly yellow, little to no brown spots\",\n",
        "            \"Dark Spots\": \"0-10%\",\n",
        "            \"Shelf Life\": \"5-7 days\",\n",
        "            \"Ripeness Stage\": \"Just ripe\",\n",
        "            \"Texture\": \"Firm and smooth\"\n",
        "        }\n",
        "    elif 5 <= distance < 9:\n",
        "        # Case 2: Banana with 40% Dark Brown Spots\n",
        "        return {\n",
        "            \"Classification\": \"Moderately Ripe\",\n",
        "            \"Freshness Index\": 6,\n",
        "            \"Color\": \"60% yellow, 40% dark spots\",\n",
        "            \"Dark Spots\": \"40% dark spots\",\n",
        "            \"Shelf Life\": \"2-3 days\",\n",
        "            \"Ripeness Stage\": \"Moderately ripe\",\n",
        "            \"Texture\": \"Some softness, still edible\"\n",
        "        }\n",
        "    else:\n",
        "        # Case 3: Almost Rotten Banana\n",
        "        return {\n",
        "            \"Classification\": \"Almost Rotten\",\n",
        "            \"Freshness Index\": 2,\n",
        "            \"Color\": \"Mostly brown or black, very few yellow patches\",\n",
        "            \"Dark Spots\": \"80-100% dark spots\",\n",
        "            \"Shelf Life\": \"0-1 days\",\n",
        "            \"Ripeness Stage\": \"Overripe\",\n",
        "            \"Texture\": \"Very soft, mushy, may leak moisture\"\n",
        "        }\n",
        "\n",
        "results = classify_banana_by_distance(distance)"
      ],
      "metadata": {
        "trusted": true,
        "id": "klumIvQLq7AC"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}